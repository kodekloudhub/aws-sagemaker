{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f1aa40f",
   "metadata": {},
   "source": [
    "# üöÄ SageMaker Data Processing Job for Tabular Data Cleanup\n",
    "\n",
    "This notebook demonstrates how to use AWS SageMaker Processing Jobs to clean tabular data from a CSV file. The key tasks include:\n",
    "\n",
    "- Removing highly correlated columns\n",
    "- Removing rows with excessive missing data\n",
    "- Imputing missing values\n",
    "- Encoding categorical features\n",
    "\n",
    "## üìö Prerequisites\n",
    "- An AWS account with SageMaker access\n",
    "- SageMaker execution role with necessary permissions\n",
    "- Input dataset in CSV format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4af3d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n",
      "SageMaker Role: arn:aws:iam::485186561655:role/service-role/AmazonSageMaker-ExecutionRole-20241205T130405\n",
      "Default Bucket: sagemaker-us-east-1-485186561655\n"
     ]
    }
   ],
   "source": [
    "# üì¶ Step 1: Setup Environment\n",
    "import sagemaker\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import os\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput, ScriptProcessor\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "print(f\"SageMaker Role: {role}\")\n",
    "print(f\"Default Bucket: {bucket}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3ae98a",
   "metadata": {},
   "source": [
    "## üìä Step 2: Explore Sample Data\n",
    "Upload your CSV dataset to the S3 bucket and explore it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a44a943d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         fullAddress  postcode  country  \\\n",
      "0  Flat 27, Rossetti Garden Mansions, Flood Stree...   SW3 5QX  England   \n",
      "1                  31 Gladstone Road, London, W4 5SX    W4 5SX  England   \n",
      "2                    53 Bexley Road, London, SE9 2PE   SE9 2PE  England   \n",
      "3  Flat 17 Long Island House, 44 Warple Way, Lond...    W3 0RG  England   \n",
      "4             51 Norwood Park Road, London, SE27 9UB  SE27 9UB  England   \n",
      "\n",
      "  outcode   latitude  longitude  bathrooms  bedrooms  floorAreaSqM  \\\n",
      "0     SW3  51.484998  -0.164776        2.0       3.0         138.0   \n",
      "1      W4  51.497203  -0.264610        1.0       2.0          80.0   \n",
      "2     SE9  51.452279   0.068871        1.0       5.0         220.0   \n",
      "3      W3  51.504021  -0.255028        NaN       NaN          74.0   \n",
      "4    SE27  51.425938  -0.095586        1.0       3.0          97.0   \n",
      "\n",
      "   livingRooms  ... saleEstimate_upperPrice saleEstimate_confidenceLevel  \\\n",
      "0          2.0  ...               2553000.0                         HIGH   \n",
      "1          1.0  ...                510000.0                         HIGH   \n",
      "2          2.0  ...               1014000.0                       MEDIUM   \n",
      "3          NaN  ...                650000.0                         HIGH   \n",
      "4          1.0  ...                757000.0                         HIGH   \n",
      "\n",
      "    saleEstimate_ingestedAt  saleEstimate_valueChange.numericChange  \\\n",
      "0  2024-10-07T13:26:59.894Z                               -379000.0   \n",
      "1  2024-10-07T13:26:59.894Z                                 16000.0   \n",
      "2  2024-10-07T13:26:59.894Z                                112000.0   \n",
      "3  2024-10-07T13:26:59.894Z                                     0.0   \n",
      "4  2024-10-07T13:26:59.894Z                                 46000.0   \n",
      "\n",
      "   saleEstimate_valueChange.percentageChange  \\\n",
      "0                                 -13.487544   \n",
      "1                                   3.404255   \n",
      "2                                  13.827160   \n",
      "3                                   0.000000   \n",
      "4                                   6.814815   \n",
      "\n",
      "   saleEstimate_valueChange.saleDate  history_date  history_price  \\\n",
      "0                         2022-11-16    2014-04-03        2100000   \n",
      "1                         2024-07-15    2020-05-29         475000   \n",
      "2                         2021-06-29    1996-11-08         178000   \n",
      "3                         2023-08-11    2016-12-09         575000   \n",
      "4                         2022-10-14    2022-10-14         675000   \n",
      "\n",
      "   history_percentageChange history_numericChange  \n",
      "0                       NaN                   NaN  \n",
      "1                 80.952381              212500.0  \n",
      "2                       NaN                   NaN  \n",
      "3                       NaN                   NaN  \n",
      "4                       NaN                   NaN  \n",
      "\n",
      "[5 rows x 28 columns]\n"
     ]
    }
   ],
   "source": [
    "# Example: Load a sample dataset\n",
    "\n",
    "df = pd.read_csv('kaggle_london_house_price_data_sampled_data.csv')  # Replace with your dataset path\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02de0c9f",
   "metadata": {},
   "source": [
    "## üìù Step 3: Create a Data Processing Script\n",
    "We'll define a Python script that will perform data cleanup tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff126936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting preprocessing_script.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile preprocessing_script.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import sys\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('/opt/ml/processing/input/data.csv')\n",
    "\n",
    "# Remove unnecessary data \n",
    "df = df.drop(columns=['fullAddress', 'rentEstimate_lowerPrice', 'rentEstimate_currentPrice', 'rentEstimate_upperPrice', \n",
    "                      'saleEstimate_lowerPrice', 'saleEstimate_upperPrice', 'outcode', 'saleEstimate_ingestedAt'])\n",
    "\n",
    "# Correct date fields and drop originals\n",
    "df['saleEstimate_valueChange.saleDate'] = pd.to_datetime(df['saleEstimate_valueChange.saleDate'])\n",
    "df['history_date'] = pd.to_datetime(df['history_date'])\n",
    "#df = df.drop(columns=['saleEstimate_valueChange.saleDate', 'history_date'])\n",
    "\n",
    "# Remove rows with excessive missing data\n",
    "df = df.dropna(thresh=int(df.shape[1] * 0.7))\n",
    "\n",
    "# Find numeric columns\n",
    "numeric_cols = df.select_dtypes(include=['number']).columns\n",
    "\n",
    "# Impute missing values for numeric columns\n",
    "num_imputer = SimpleImputer(strategy='mean')\n",
    "df[numeric_cols] = num_imputer.fit_transform(df[numeric_cols])\n",
    "\n",
    "# Target Encoding postcode and current price instead of One Host Encoding due to high cardinality\n",
    "target_mean = df.groupby('postcode')['saleEstimate_currentPrice'].mean()\n",
    "df['Postcode_Encoded'] = df['postcode'].map(target_mean)\n",
    "\n",
    "# Drop original Postcode column as no longer needed\n",
    "df = df.drop(columns=['postcode'])\n",
    "\n",
    "# Encode categorical columns\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "df = pd.get_dummies(df, columns=categorical_cols)\n",
    "\n",
    "# Save processed data\n",
    "df.to_csv('/opt/ml/processing/output/processed.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdcdeae-056b-4743-81ff-1bfde959af37",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Step 4: Upload Source Data to S3\n",
    "Upload the source CSV dataset to input location in S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cf3911b-a529-4dd5-9d24-b00b11e3903d",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "s3.meta.client.upload_file('kaggle_london_house_price_data_sampled_data.csv', bucket, 'input/data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153a6031",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Step 5: Run SageMaker Processing Job\n",
    "Run the preprocessing script using a SageMaker Processing Job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "596c3c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Defaulting to only available Python version: py3\n",
      "INFO:sagemaker.image_uris:Defaulting to only supported image scope: cpu.\n",
      "INFO:sagemaker:Creating processing-job with name sagemaker-scikit-learn-2025-01-13-00-25-49-668\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...............\n",
      ".."
     ]
    }
   ],
   "source": [
    "input_raw_data_prefix = \"input/\"\n",
    "output_preprocessed_data_prefix = \"output\"\n",
    "\n",
    "processor = ScriptProcessor(\n",
    "    image_uri=sagemaker.image_uris.retrieve('sklearn', 'us-east-1', '1.2-1'),\n",
    "    role=role,\n",
    "    command=['python3'],\n",
    "    instance_type='ml.m5.4xlarge',\n",
    "    instance_count=1\n",
    ")\n",
    "\n",
    "processor.run(\n",
    "    code='preprocessing_script.py',\n",
    "    inputs=[ProcessingInput(source=\"s3://\" + os.path.join(bucket, input_raw_data_prefix, \"data.csv\"),\n",
    "                            destination='/opt/ml/processing/input')], \n",
    "    outputs=[ProcessingOutput(source='/opt/ml/processing/output',\n",
    "                            destination=\"s3://\" + os.path.join(bucket, output_preprocessed_data_prefix, \"data-processed.csv\"))]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6acbbb7b",
   "metadata": {},
   "source": [
    "## üì• Step 6: Validate Cleaned Data\n",
    "Download the processed data from S3 and validate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9e3896",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = pd.read_csv(f's3://{bucket}/output/processed.csv')\n",
    "print(df_cleaned.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
