{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f1aa40f",
   "metadata": {},
   "source": [
    "# üöÄ SageMaker Data Processing Job for Tabular Data Cleanup\n",
    "\n",
    "This notebook demonstrates how to use AWS SageMaker Processing Jobs to clean tabular data from a CSV file. The key tasks include:\n",
    "\n",
    "- Removing highly correlated columns\n",
    "- Removing rows with excessive missing data\n",
    "- Imputing missing values\n",
    "- Encoding categorical features\n",
    "\n",
    "## üìö Prerequisites\n",
    "- An AWS account with SageMaker access\n",
    "- SageMaker execution role with necessary permissions\n",
    "- Input dataset in CSV format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4af3d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì¶ Step 1: Setup Environment\n",
    "import sagemaker\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import os\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput, ScriptProcessor\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "print(f\"SageMaker Role: {role}\")\n",
    "print(f\"Default Bucket: {bucket}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3ae98a",
   "metadata": {},
   "source": [
    "## üìä Step 2: Explore Sample Data\n",
    "Upload your CSV dataset to the S3 bucket and explore it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44a943d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Load a sample dataset\n",
    "\n",
    "df = pd.read_csv('kaggle_london_house_price_data_sampled_data.csv')  # Replace with your dataset path\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02de0c9f",
   "metadata": {},
   "source": [
    "## üìù Step 3: Create a Data Processing Script\n",
    "We'll define a Python script that will perform data cleanup tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff126936",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile preprocessing_script.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import sys\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('/opt/ml/processing/input/data.csv')\n",
    "\n",
    "# Remove unnecessary data \n",
    "df = df.drop(columns=['fullAddress', 'rentEstimate_lowerPrice', 'rentEstimate_currentPrice', 'rentEstimate_upperPrice', \n",
    "                      'saleEstimate_lowerPrice', 'saleEstimate_upperPrice', 'outcode', 'saleEstimate_ingestedAt'])\n",
    "\n",
    "# Correct date fields and drop originals\n",
    "df['saleEstimate_valueChange.saleDate'] = pd.to_datetime(df['saleEstimate_valueChange.saleDate'])\n",
    "df['history_date'] = pd.to_datetime(df['history_date'])\n",
    "#df = df.drop(columns=['saleEstimate_valueChange.saleDate', 'history_date'])\n",
    "\n",
    "# Remove rows with excessive missing data\n",
    "df = df.dropna(thresh=int(df.shape[1] * 0.7))\n",
    "\n",
    "# Find numeric columns\n",
    "numeric_cols = df.select_dtypes(include=['number']).columns\n",
    "\n",
    "# Impute missing values for numeric columns\n",
    "num_imputer = SimpleImputer(strategy='mean')\n",
    "df[numeric_cols] = num_imputer.fit_transform(df[numeric_cols])\n",
    "\n",
    "# Target Encoding postcode and current price instead of One Host Encoding due to high cardinality\n",
    "target_mean = df.groupby('postcode')['saleEstimate_currentPrice'].mean()\n",
    "df['Postcode_Encoded'] = df['postcode'].map(target_mean)\n",
    "\n",
    "# Drop original Postcode column as no longer needed\n",
    "df = df.drop(columns=['postcode'])\n",
    "\n",
    "# Encode categorical columns\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "df = pd.get_dummies(df, columns=categorical_cols)\n",
    "\n",
    "# Save processed data\n",
    "df.to_csv('/opt/ml/processing/output/processed.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdcdeae-056b-4743-81ff-1bfde959af37",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Step 4: Upload Source Data to S3\n",
    "Upload the source CSV dataset to input location in S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cf3911b-a529-4dd5-9d24-b00b11e3903d",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "s3.meta.client.upload_file('kaggle_london_house_price_data_sampled_data.csv', bucket, 'input/data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153a6031",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Step 5: Run SageMaker Processing Job\n",
    "Run the preprocessing script using a SageMaker Processing Job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596c3c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_raw_data_prefix = \"input/\"\n",
    "output_preprocessed_data_prefix = \"output\"\n",
    "\n",
    "processor = ScriptProcessor(\n",
    "    image_uri=sagemaker.image_uris.retrieve('sklearn', 'us-east-1', '1.2-1'),\n",
    "    role=role,\n",
    "    command=['python3'],\n",
    "    instance_type='ml.t3.medium',\n",
    "    instance_count=1\n",
    ")\n",
    "\n",
    "processor.run(\n",
    "    code='preprocessing_script.py',\n",
    "    inputs=[ProcessingInput(source=\"s3://\" + os.path.join(bucket, input_raw_data_prefix, \"data.csv\"),\n",
    "                            destination='/opt/ml/processing/input')], \n",
    "    outputs=[ProcessingOutput(source='/opt/ml/processing/output',\n",
    "                            destination=\"s3://\" + os.path.join(bucket, output_preprocessed_data_prefix, \"data-processed\"))]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6acbbb7b",
   "metadata": {},
   "source": [
    "## üì• Step 6: Validate Cleaned Data\n",
    "Download the processed data from S3 and validate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9e3896",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = pd.read_csv(f's3://{bucket}/output/data-processed/processed.csv')\n",
    "print(df_cleaned.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
